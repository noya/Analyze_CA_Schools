---
title: "A Study of Factors Predicting School Performance Over Time"
author: "cindyst2, trapido2, bching3, rjs8"
date: '07/30/2018'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

## Introduction

This is a study of factors predicting school performance over time. There are four separate years being analyzed, 2004, 2006, 2010, and 2013. We want to understand what factors affect school performance and whether those factors have changed over time.

## Dataset Description

The dataset tracks the school's name, district, type (charter or not), student demographics, subject end of year test score averages, ranking in the state, class sizes, average parental education levels, and teacher credentials.

With the performance index as the response, we hope to model what predictors contribute to the performance of a school.

TODO: combine text 

## The Data Source

The file used for this analysis is at https://www.cde.ca.gov/ta/ac/ap/apidatafiles.asp, specifically, the 2004, 2006, 2010, and 2013 Growth APIs.

## Background

The data tracks school performance as recorded by the CA Dept. of Education from 1999-2013 (https://www.cde.ca.gov/ta/ac/ap/apidatafiles.asp)

## Interest

Interest in this dataset is driven by the current administration's Education Secretary's push to increase the number of charter schools.

## Methods

We will use API scores as our response. Since we are observing 2004, 2007, 2010, and 2013, we will use the API scores from these respective years as our response. For each year we will fit a model to the API score.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(ggplot2)
library(latex2exp)
library(lmtest)    # provides bptest()
library(knitr)     # provides kable()
library(readr)
library(gridExtra)
library(car)       # provides car
library(caret)
```

```{r eval=FALSE,include=FALSE}
install.packages("foreign")
```

```{r}
set.seed(42)
data_2013 = read.dbf("data/api13gdb.dbf")
data_2010 = read.dbf("data/api10gdb.dbf", as.is = TRUE)
data_2007 = read.dbf("data/api07gdb.dbf", as.is = TRUE)
data_2004 = read.dbf("data/api04gdb.dbf", as.is = TRUE)
```

```{r eval=FALSE,include=FALSE}
View(data_2013)
```


## Response

The response variable for this analysis is going to be the corresponding `APInn` column for that year.

## Predictors

We start out by looking at the data and removing any rows that have missing data since we would be unable to provide estimates for their values.

```{r}
#
# Utility functions to manipulate the data.
#

# Function to remove exact column names
remove_columns = function(dframe, columns_to_remove) {
  subset(dframe, select = !(colnames(dframe) %in% columns_to_remove))
}

# Function to remove matching column names
remove_columns_like = function(dframe, columns_to_remove) {
  tmp_df = data.frame(dframe)
  for(match in columns_to_remove) {
    tmp_df = subset(tmp_df, select = -grep(match, colnames(tmp_df), perl=TRUE, value=FALSE))
  }
  tmp_df
}

# threshold is a number from 0 to 1 to indicate how much NA you want to see
# threshold = 0.7 means show columns with 70% NAs
show_columns_with_na = function(dframe, threshold = 0.5) {
  cn = colnames(dframe)
  col_na_ratios = rep(0, length(cn))
  for(c in 1:length(cn)) {
    col_na_ratios[c] = mean(is.na(dframe[,cn[c]]))
  }
  print(cn[col_na_ratios > threshold])
  cn[col_na_ratios > threshold]
}

# Remove columns common in dataset
remove_common_columns = function(dframe) {
  
  # Remove all _TARG, _MET, _SIG, columns which are just metadata about whether targets were met
  # or whether columns were significant.
  dframe = remove_columns_like(dframe, c("_TARG", "_MET", "_SIG", "_NUM", "_GROW"))

  # Remove rows with YR_RND = Yes because they have mostly NA columns
  dframe = subset(dframe, subset = dframe$YR_RND != 'Yes', select =!(colnames(dframe) %in% c('YR_RND')))

  # Remove the names because we have the school id in the first column
  dframe = remove_columns(dframe, c("SNAME", "DNAME", "CNAME"))

  # Remove rows with AVG_ED = NA because there's not that much of them (37)
  dframe = subset(dframe, subset = !is.na(dframe$AVG_ED))

  # Remove ACS_K3, ACS_46, ACS_CORE, FULL, EMER because it's mostly if not all is NA
  dframe = remove_columns(dframe, c("ACS_K3", "ACS_46", "ACS_CORE", "FULL", "EMER"))

  dframe
}

# Remove columns that are only there in 3 instead of 4 datasets
remove_less_common = function(dframe) {
  dframe$CHARTER = ifelse(is.na(dframe$CHARTER), "NC", dframe$CHARTER)

  # Transform SPED column by setting NA to 'R' for regular b/c it uses NA to indicate regular schools
  dframe$SPED = as.factor(ifelse(is.na(dframe$SPED), "R", dframe$SPED))
  
  # Remove schools with SIZE = S or T which indicate non-valid API scores
  dframe = subset(dframe, subset = !(dframe$SIZE %in% c("S","T")), select =!(colnames(dframe) %in% c('SIZE')))

  # Transform STYPE column by setting NA to 'O' to indicate other
  dframe$STYPE = as.factor(ifelse(is.na(dframe$STYPE), "O", dframe$STYPE))
}

# convert all columns listed in fac_pred to factors
conv_factor = function(dframe, columns_to_factor) {
  for(col in columns_to_factor) {
    if(col %in% colnames(dframe)) {
      dframe[, col] = as.factor(dframe[, col])
    }
  }
  dframe
}

# convert all columns to numeric
conv_numeric = function(dframe, columns_to_numeric) {
  for(col in columns_to_numeric) {
    if(col %in% colnames(dframe)) {
      dframe[, col] = as.numeric(dframe[, col])
    }
  }
  dframe
}

# Function to calculate loocv_rmse
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# Function to plot qqplot and perform shapiro and bptest
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, 
                       plotit = TRUE, testit = TRUE) {
  
  if (plotit == TRUE) {
    
    # side-by-side plots (one row, two columns)
    par(mfrow = c(1, 2))
    
    # fitted versus residuals
    plot(fitted(model), resid(model), 
         col = pcol, pch = 20, cex = 1.5, 
         xlab = "Fitted", ylab = "Residuals", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    grid()
    
    # qq-plot
    qqnorm(resid(model), col = pcol, pch = 20, cex = 1.5)
    qqline(resid(model), col = lcol, lwd = 2)
    grid()
  }
  
  if (testit == TRUE) {
    # p-value and decision
    shapiro_pval = shapiro.test(resid(model))$p.value
    bptest_pval = bptest((model))$p.value
    list(shapiro_pval = shapiro_pval, bptest_pval = bptest_pval)
  }

}
```

```{r}
###################
# Common Columns
###################

# Define some common columns
# Factor Columns
fact_columns = c('RTYPE', 'STYPE', 'SPED', 'SIZE', 'CHARTER', "YR_RND",
                 "SCH_WIDE" , "COMP_IMP ", "BOTH", "AWARDS")

# Numeric Columns
num_columns = c(
             "API10", "API13", "API07", "API04",
             "API09", 
             'MEDIAN13','MEDIAN12','MEDIAN10','MEDIAN09','MEDIAN07','MEDIAN06','MEDIAN04','MEDIAN03',
             "ST_RANK", "SIM_RANK", 'sim_rank','st_rank', "SCI",
             "CDS",'VALID',
             "AA_API", "AI_API", "AS_API", "FI_API", "HI_API", "PI_API", "WH_API", "MR_API",
             "SD_API", "EL_API", "DI_API",
             "PCT_AA", "PCT_AI", "PCT_AS", "PCT_FI", "PCT_HI","PCT_PI", "PCT_WH", "PCT_MR",
             "MEALS", "P_GATE",  "P_MIGED", "P_EL", "P_RFEP", "P_DI", "CBMOB", "DMOB", 
             "ACS_K3", "ACS_46", "ACS_CORE", 
             "PCT_RESP", "NOT_HSG", "HSG", "SOME_COL", "COL_GRAD", "GRAD_SCH", "AVG_ED",
             "PEN_2", "PEN_35", "PEN_6", "PEN_78", "PEN_911","PEN_91",
             "ENROLL", "TESTED", 
             'VCST_E28','PCST_E28','VCST_E91','PCST_E91','CW_CSTE',
             'VCST_M28','PCST_M28','VCST_M91','PCST_M91','CW_CSTM',
             'VCST_S28','PCST_S28','VCST_S91','PCST_S91','CWS_91',
             'VCST_H28','PCST_H28','VCST_H91','PCST_H91','CW_CSTH',
             'VCSTM2_91','PCSTM2_91','CWM2_91','CWS2_91','VCSTS2_91','PCSTS2_91',
             'VCST_LS10','PCST_LS10','CWM2_28','VCSTM2_28','PCSTM2_28',
              'VCHS_E','PCHS_E','CW_CHSE',
             'VCHS_M','PCHS_M','CW_CHSM',
              'TOT_28','TOT_91','CW_SCI',
             "CAPA", "PAR_OPT", "VALID", "GROWTH","API09","TARGET","PAR_OPT","MEDIAN09"
             )

```

```{r}
#############################
# Data Cleaning for 2004
#############################
# remove comoonly removable columns
data_2004 = remove_common_columns(data_2004)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2004 = remove_columns_like(data_2004, c("_API04", "_API03"))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2004 = subset(data_2004, subset = !is.na(data_2004$MEDIAN04) | !is.na(data_2004$MEDIAN03))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)
other_cols = show_columns_with_na(data_2004)
data_2004 = remove_columns(data_2004, other_cols)
data_2004 = na.omit(data_2004)

# convert columns to factor
data_2004 = conv_factor(data_2004, fact_columns)

# convert columns to numeric
data_2004 = conv_numeric(data_2004, num_columns)
nrow(data_2004)
ncol(data_2004)

```


```{r}
#############################
# Data Cleaning for 2007
#############################
# remove comoonly removable columns
data_2007 = remove_common_columns(data_2007)

data_2007$CHARTER = ifelse(is.na(data_2007$CHARTER), "NC", data_2007$CHARTER)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2007 = remove_columns_like(data_2007, c("_API07", "_API06"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2007 = subset(data_2007, subset = is.na(data_2007$sm07) & is.na(data_2007$sm06), select = !(colnames(data_2007) %in% c('sm07','sm06')))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2007 = subset(data_2007, subset = !is.na(data_2007$MEDIAN07) | !is.na(data_2007$MEDIAN06))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)
other_cols = show_columns_with_na(data_2007)
data_2007 = remove_columns(data_2007, other_cols)
data_2007 = na.omit(data_2007)

# convert columns to factor
data_2007 = conv_factor(data_2007, fact_columns)

# convert columns to numeric
data_2007 = conv_numeric(data_2007, num_columns)
nrow(data_2007)
ncol(data_2007)

#############################
# Data Cleaning for 2010
#############################
# remove comoonly removable columns
data_2010 = remove_common_columns(data_2010)

data_2010$CHARTER = ifelse(is.na(data_2010$CHARTER), "NC", data_2010$CHARTER)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2010 = remove_columns_like(data_2010, c("_api10", "_API09"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2010 = subset(data_2010, subset = is.na(data_2010$sm10) & is.na(data_2010$sm09), select = !(colnames(data_2010) %in% c('sm10','sm09')))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2010 = subset(data_2010, subset = !is.na(data_2010$MEDIAN10) | !is.na(data_2010$MEDIAN09))

# Remove all fields with component test scores because this is what we're trying to find relationships for.
# TODO can move to common remove column
data_2010 = remove_columns_like(data_2010, c("VCST_", "PCST_", "VCHS_", "PCHS", "CW_", "CWM2_", "CWS2_", "VCSTM2_", "PCSTM2_", "CWS_", "VCSTS2_", "PCSTS2_", "TOT_"))

# also related to API
data_2010 = remove_columns(data_2010, c("API09", "TARGET", "GROWTH", "COMP_IMP", "PAR_OPT", "MEDIAN10", "MEDIAN09", "VALID"))
# Remove columns with too many NA
other_cols = show_columns_with_na(data_2010)
data_2010 = remove_columns(data_2010, other_cols)
data_2010 = na.omit(data_2010)

# convert columns to factor
data_2010 = conv_factor(data_2010, fact_columns)

# convert columns to numeric
data_2010 = conv_numeric(data_2010, num_columns)

nrow(data_2010)
```



```{r}
# group 13
set.seed(13)
#
# Randomly draw 600 samples from dataset
# Split samples into train and test
#

# 2004
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2004), 600)
data_2004 = data_2004[ridx, ]

# split dataframe into training and test
(n = nrow(data_2004))
trn_idx = sample(nrow(data_2004), n/2)
data_2004_trn = data_2004[trn_idx, ]
data_2004_tst = data_2004[-trn_idx, ]

# 2007
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2007), 600)
data_2007 = data_2007[ridx, ]

# split dataframe into training and test
(n = nrow(data_2007))
trn_idx = sample(nrow(data_2007), n/2)
data_2007_trn = data_2007[trn_idx, ]
data_2007_tst = data_2007[-trn_idx, ]

# 2010
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2010), 600)
data_2010 = data_2010[ridx, ]

# split dataframe into training and test
(n = nrow(data_2010))
trn_idx = sample(nrow(data_2010), n/2)
data_2010_trn = data_2010[trn_idx, ]
data_2010_tst = data_2010[-trn_idx, ]

```


```{r}
#################
# 2004
#################
#   
# mod_2004 = lm(API04  ~ PCT_AS +  log(PCT_HI)  + PCT_WH   +   DMOB + PCT_RESP +  (AVG_ED) +  ENROLL , data = data_2004_trn )
# 
# 
# # check assumptions
# diagnostics(mod_2004)
# 
# # check how the model is fitting
# summary(mod_2004)
# vif(mod_2004) 
# calc_loocv_rmse(mod_2004)
# 
# pred = predict(mod_2004, newdata = data_2004_tst)
# plot(pred ~ mod_2004$API04, col = "dodgerblue")

#################
# 2007
#################
# schl2010_fit = lm(API10  ~ PEN_2  +  PEN_35 + PEN_6   + PEN_78  + poly(PCT_AA, 2) + poly(PCT_AI, 1) + poly(PCT_AS, 2) + poly(PCT_WH, 2) + poly(P_GATE, 1) + P_MIGED + poly(AVG_ED, 4) + poly(MEALS, 1) ,
#                data = data_2010_trn)
  
#  mod_2007 = lm(API07  ~ PEN_2 +  PEN_35 +  PCT_AS +  log(PCT_HI)  + PCT_WH     + P_GATE +    P_EL  +   DMOB + PCT_RESP +  poly(AVG_ED, 3) +  ENROLL , data = data_2007_trn )
#   
#   
# # check assumptions
# diagnostics(mod_2007)
# 
# # check how the model is fitting
# summary(mod_2007)
# vif(mod_2007) 
# calc_loocv_rmse(mod_2007)
# 
# pred = predict(mod_2007, newdata = data_2007_tst)
# plot(pred ~ schl2010_tst$API07, col = "dodgerblue")

#################
# 2010
#################
# mod_2010 = lm(API10  ~ PEN_2  +  PEN_35 + PEN_6   + PEN_78  + poly(PCT_AA, 2) + poly(PCT_AI, 1) + poly(PCT_AS, 2) + poly(PCT_WH, 2) + poly(P_GATE, 1) + P_MIGED + poly(AVG_ED, 4) + poly(MEALS, 1) ,
#                data = data_2004_trn)
  
 mod_2010 = lm(API10  ~ PEN_2 +  PEN_35 +  poly(PCT_AS, 2) +  (PCT_HI)  + PCT_WH     + P_GATE +    P_EL  +   DMOB + PCT_RESP +  poly(AVG_ED, 3) +  ENROLL + MEALS, data = data_2010_trn )


# check assumptions
diagnostics(mod_2010)

# check how the model is fitting
summary(mod_2010)
vif(mod_2010)
calc_loocv_rmse(mod_2010)

pred = predict(mod_2010, newdata = data_2010_tst)
plot(pred ~ data_2010_tst$API10, col = "dodgerblue")

```

## Example of Detailed Analysis for 2013

The following R code is a little bit more detailed to show some of the thought process and iterations we had to go through to clean our data.

```{r}
data_2013_raw = read.dbf("data/api13gdb.dbf")

# 2013


#############################
# Data Cleaning for 2013 -- Shows some of the thought process that went into cleaning our data.
#############################
# First get the subset that the documentation says has valid statistical data,
# then remove the FLAG column because we don't need it anymore.
# Also remove the other alternative responses related to growth and APIs.
data_2013 = subset(data_2013_raw, subset = is.na(data_2013$FLAG), select = !(colnames(data_2013) %in% c('FLAG','API12','TARGET','GROWTH','SCH_WIDE','COMP_IMP','BOTH')))

# Remove all _TARG, _MET, _SIG, _API12, _API13 columns which are just metadata about whether targets were met
# or whether columns were significant.
data_2013 = remove_columns_like(data_2013, c("_TARG", "_MET", "_SIG", "_API12", "_API13", "_NUM", "_GROW"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2013 = subset(data_2013, subset = is.na(data_2013$SM12) & is.na(data_2013$SM13), select = !(colnames(data_2013) %in% c('SM12','SM13')))

# Remove schools with SIZE = S or T which indicate non-valid API scores
data_2013 = subset(data_2013, subset = !(data_2013$SIZE %in% c("S","T")), select =!(colnames(data_2013) %in% c('SIZE')))

# Transform CHARTER column to indicate Y or N because it uses NA to indicate a non-charter school
data_2013$CHARTER = as.factor(ifelse(is.na(data_2013$CHARTER), "N", "Y"))


# Transform SPED column by setting NA to 'R' for regular b/c it uses NA to indicate regular schools
data_2013$SPED = as.factor(ifelse(is.na(data_2013$SPED), "R", data_2013$SPED))

# Transform STYPE column by setting NA to 'O' to indicate other
data_2013$STYPE = as.factor(ifelse(is.na(data_2013$STYPE), "O", data_2013$STYPE))

# Remove the names because we have the school id in the first column
data_2013 = remove_columns(data_2013, c("SNAME", "DNAME", "CNAME"))

# Remove rows with YR_RND = Yes because they have mostly NA columns
data_2013 = subset(data_2013, subset = data_2013$YR_RND != 'Yes', select =!(colnames(data_2013) %in% c('YR_RND')))

# Remove rows with AVG_ED = NA because there's not that much of them (37)
data_2013 = subset(data_2013, subset = !is.na(data_2013$AVG_ED))

# Remove rows with st_rank and sim_rank = NA b/c there's not that much of them (1 each)
data_2013 = subset(data_2013, subset = !is.na(data_2013$st_rank) & !is.na(data_2013$sim_rank))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2013 = subset(data_2013, subset = !is.na(data_2013$MEDIAN13) | !is.na(data_2013$MEDIAN12))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout 
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)

other_cols = show_columns_with_na(data_2013, 0)
data_2013 = remove_columns(data_2013, other_cols)

# Convert non-factor columns to numeric.
num_columns = c('CDS','VALID','API13','PCT_AA','PCT_AI','PCT_AS','PCT_FI','PCT_HI','PCT_PI','PCT_WH',
'PCT_MR','MEALS','P_GATE','P_MIGED','P_EL','P_RFEP','P_DI','CBMOB','DMOB','ACS_K3',
'ACS_46','ACS_CORE','PCT_RESP','NOT_HSG','HSG','SOME_COL','COL_GRAD','GRAD_SCH','AVG_ED','FULL',
'EMER','PEN_2','PEN_35','PEN_6','PEN_78','PEN_911','ENROLL','PAR_OPT','TESTED','MEDIAN13',
'MEDIAN12','VCST_E28','PCST_E28','VCST_E91','PCST_E91','CW_CSTE','VCST_M28','PCST_M28','VCST_M91','PCST_M91',
'CW_CSTM','VCST_S28','PCST_S28','VCST_S91','PCST_S91','CWS_91','VCST_H28','PCST_H28','VCST_H91','PCST_H91',
'CW_CSTH','VCHS_E','PCHS_E','CW_CHSE','VCHS_M','PCHS_M','CW_CHSM','TOT_28','TOT_91','CW_SCI',
'VCST_LS10','PCST_LS10','CWM2_28','VCSTM2_28','PCSTM2_28','CWM2_91','VCSTM2_91','PCSTM2_91','CWS2_91','VCSTS2_91',
'PCSTS2_91','sim_rank','st_rank')
for(c in 1:length(num_columns)) {
  data_2013[,num_columns[c]] = as.numeric(data_2013[,num_columns[c]])
}

# remove school identifier
data_2013 = remove_columns(data_2013, c("CDS"))

unique(data_2013$RTYPE) # yields only one factor in dataset, so remove the column
data_2013 = remove_columns(data_2013, c("RTYPE"))

# Remove all fields with component test scores because this is what we're trying to find relationships for.
data_2013 = remove_columns(data_2013, c('TESTED','MED13','MED12','VCST_E28','PCST_E28','VCST_E91','PCST_E91','VCST_M28','PCST_M28','VCST_M91',
'PCST_M91','VCST_S28','PCST_S28','VCST_S91','PCST_S91','VCST_H28','PCST_H28','VCST_H91','PCST_H91','VCHS_E',
'PCHS_E','VCHS_M','PCHS_M','TOT_28','TOT_91','MEDIAN13','MEDIAN12','chg_data','VCST_LS10','PCST_LS10',
'VCSTM2_28','PCSTM2_28','173VCSTM2_91','174PCSTM2_91','VCSTS2_91','PCSTS2_91','IRG5','sim_rank','st_rank'))
data_2013 = remove_columns(data_2013, c("VCSTM2_91","PCSTM2_91","SPED"))

# split dataframe into training and test
(n = nrow(data_2013))
trn_idx = sample(nrow(data_2013), n/2)
data_2013_trn = data_2013[trn_idx, ]
data_2013_tst = data_2013[-trn_idx, ]

```

```{r}
model_initial = lm(API13 ~ ., data=data_2013_trn)
model_sel = step(model_initial, trace = 0)
```

### Tests for Normality

We test the model for normality to make sure our model is valid.

QQPlots
```{r}
qqnorm(resid(model_sel))
qqline(resid(model_sel))
plot(resid(model_sel), fitted(model_sel))
```

BPTest and Shapiro Test
```{r}
bptest(model_sel)
shapiro.test(resid(model_sel)[1:3000])
```

The tests for normality fail. We must now see if we can reduce the model to one that is normally distributed.

### Tests for Collinearity

We look at the `pairs` plot to see if any of the predictors are collinear. The output has been commented out for brevity for `pairs` charts that didn't show any relationships.

```{r}
#pairs(subset(data_2013_trn,select=c('API13', 'VALID', 'PCT_AA', 'PCT_AI')))
#pairs(subset(data_2013_trn,select=c('API13', 'PCT_AS', 'PCT_FI', 'PCT_HI')))
#pairs(subset(data_2013_trn,select=c('API13', 'PCT_PI', 'PCT_WH', 'PCT_MR')))
#pairs(subset(data_2013_trn,select=c('API13', 'MEALS', 'P_MIGED', 'P_DI')))
pairs(subset(data_2013_trn,select=c('API13', 'CBMOB', 'DMOB', 'ACS_K3')))
#pairs(subset(data_2013_trn,select=c('API13', 'ACS_46', 'ACS_CORE', 'NOT_HSG')))
#pairs(subset(data_2013_trn,select=c('API13', 'HSG', 'SOME_COL', 'GRAD_SCH')))
#pairs(subset(data_2013_trn,select=c('API13', 'AVG_ED', 'FULL', 'EMER')))
pairs(subset(data_2013_trn,select=c('API13', 'PEN_2', 'PEN_35', 'PEN_6')))
#pairs(subset(data_2013_trn,select=c('API13', 'PEN_78', 'PEN_911', 'ENROLL')))
#pairs(subset(data_2013_trn,select=c('API13', 'PAR_OPT', 'CW_CSTE', 'CWS_91')))
#pairs(subset(data_2013_trn,select=c('API13', 'CW_CSTH', 'CW_CHSM', 'CW_SCI')))
```

It looks like the following fields may be collinear: DMOB with CBMOB, PEN_2 with PEN_35, PEN_6, PEN_78, and PEN_911. The collinear columns will be removed.

The following fields look like they can use a `log()` transform: PCT_AA, PCT_AS, P_MIGED, P_DI, CBMOB, and FULL. 

### Striated Sub-Sampling

```{r}
# Apply backwards AIC to the given model and plot the QQPlot and residual vs fitted
model_it = function(a_model) {
  opt_model = step(a_model, trace=0)
  qqnorm(resid(opt_model))
  qqline(resid(opt_model))
  plot(resid(opt_model), fitted(opt_model))
  opt_model
}

# Data still not passing normality tests.
# Try striated partitions: 30 random samples for range 11-598
randomly_select = function(dframe, no_to_select) {
  dframe[sample(nrow(dframe), no_to_select), ] # it's up to the caller to make sure there are enough rows
}

striation_subsample = function(dframe, response, no_bins = 20, bin_size = 30) {
  range_response = range(dframe[, response])
  response_interval = (range_response[2] - range_response[1])/no_bins
  no_breaks = seq(range_response[1], range_response[2], by=response_interval)
  
  reduced_data = data.frame(dframe[0,]) # First add the column headers only
  for(i in seq(range_response[1], range_response[2], by=response_interval)) {
    interval_rows = dframe[, response] >= i & dframe[, response] < (i + response_interval)

    no_rows_in_interval = nrow(dframe[interval_rows,])

    print(paste(i,' => ',no_rows_in_interval,' elts'))
    if (no_rows_in_interval > bin_size) {
      reduced_data = rbind(reduced_data, 
                           randomly_select(subset(dframe, subset = interval_rows),
                                           bin_size))
    } else { # add the entire bin to the resulting dataset
      reduced_data = rbind(reduced_data,
                           subset(dframe, subset = interval_rows))
    }
  }
  print(paste('Reduced model has',nrow(reduced_data),'rows'))
  reduced_data
}

hist(data_2013_trn$API13)
reduced_data = striation_subsample(data_2013_trn, 'API13', 20, 30)
hist(reduced_data$API13)
```


```{r}
# Test reduced model; STYPE and VALID showed high collinearity in a VIF output so those were removed
model_init = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_AI + 
                  log(PCT_AS) + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
                  log(P_MIGED) + log(P_DI) + log(CBMOB) + ACS_K3 + ACS_46 + ACS_CORE + 
                  NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + log(FULL) + EMER + 
                  ENROLL + PAR_OPT + 
                  CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = reduced_data)
print("Model Optimization 1")
model_opt1 = model_it(model_init)
summary(model_opt1)
bptest(model_opt1)
shapiro.test(resid(model_opt1))

print("After cooks distance applied")
cd = cooks.distance(model_opt1)
no_outlier_data = subset(reduced_data, subset = cd < 3/length(cd)) # using an aggressive cooks distance criteria
print(paste(nrow(no_outlier_data),"rows left"))

model_init2 = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_AI + 
                   log(PCT_AS) + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
                   log(P_MIGED) + log(P_DI) + log(CBMOB) + ACS_K3 + ACS_46 + ACS_CORE + 
                   NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + log(FULL) + EMER + 
                   ENROLL + PAR_OPT + 
                   CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = no_outlier_data)
print("Model Optimization 2")
model_opt2 = model_it(model_init2)
summary(model_opt2)
bptest(model_opt2)
shapiro.test(resid(model_opt2))

```
```{r}
# check collinearity
car::vif(model_opt2)
```


## Conclusions 2013

Even though the bptest rejected the null hypothesis of a constant variance, the `Fitted vs Residual` plot doesn't show any discernible patterns. The Shapiro test failed to reject the null hypothesis that the data was sampled from a normal distribution. The `vif()` check doesn't show collinearity for any of the final set of predictors. The QQ-Plot looks good.

When this script is run repeatedly, there are two large influencers that emerge. One case looks like this:
```{r}
final1 = lm(API13 ~ log(PCT_AA) + PCT_FI + PCT_HI + PCT_PI + 
              log(P_MIGED) + log(CBMOB) + ACS_46 + ACS_CORE + HSG + AVG_ED + 
              EMER + CWS_91 + CW_CSTH + CW_CHSM, data=no_outlier_data)
bptest(final1)
shapiro.test(resid(final1))
qqnorm(resid(final1))
qqline(resid(final1))
plot(resid(final1), fitted(final1))
summary(final1)$adj  
coef(final1)
```

The other case looks like this:
```{r}
final2 = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_FI + PCT_HI + 
              PCT_PI + log(P_DI) + ACS_46 + ACS_CORE + GRAD_SCH + AVG_ED + 
              EMER + CWS_91 + CW_CHSM + CW_SCI, data=no_outlier_data)
bptest(final2)
shapiro.test(resid(final2))
qqnorm(resid(final2))
qqline(resid(final2))
plot(resid(final2), fitted(final2))
summary(final2)$adj 
coef(final2)
```

Can we predict?

```{r}
pred = predict(final2, newdata = data_2013_tst)
plot(pred ~ data_2013_tst$API13, col = "dodgerblue")
```



In the end, the largest influencers of performance are:

* CBMOB - whether the student doesn't skip school day for more than 30 consecutive days from when the school starts
* AVG_ED - the parent's education level
* ACS_CORE - the class size
* CHARTER - whether the school is a charter school or not
* PCT_AA and PCT_PI - the presence of some disadvantaged minority and ethnic groups

It's important to note that these influencers illustrate correlation and not causation.






